{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "open_qa_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1t2gVb9nHaNvqXMWXgfQnt_4Rn22J-qWe",
      "authorship_tag": "ABX9TyOhZH22YrHNE6v6QnlaIuRZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhansoll/nlp_tutorials/blob/main/open_qa_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYVz_mZu1ofR"
      },
      "source": [
        "!pip install --quiet faiss-cpu\n",
        "!pip install --quiet tensorflow\n",
        "!pip install --quiet tensorflow_hub\n",
        "!pip install --quiet tf-models-official"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcSqjkUt1tDZ"
      },
      "source": [
        "import pickle\n",
        "import faiss\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from official.nlp.bert import tokenization"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJyH-Nu_2Rte"
      },
      "source": [
        "BERT_MODEL = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\"\n",
        "\n",
        "tfhub_handle_passage_encoder = hub.load(BERT_MODEL)\n",
        "tfhub_handle_question_encoder = hub.load(BERT_MODEL)\n",
        "tfhub_handle_reader_encoder = hub.load(BERT_MODEL)\n",
        "\n",
        "vocab_file = tfhub_handle_passage_encoder.vocab_file.asset_path.numpy()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case=tfhub_handle_passage_encoder.do_lower_case)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7STXPTB8H_W",
        "outputId": "67a12cbf-c6bb-4312-b876-debb95abbfaa"
      },
      "source": [
        "tfhub_handle_passage_encoder.do_lower_case"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=() dtype=bool, numpy=True>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_Sf3akq77lA"
      },
      "source": [
        "`Uncased` means that the text has been lowercased before WordPiece tokenization, e.g., John Smith becomes john smith. The `Uncased` model also strips out any accent markers. `Cased` means that the true case and accent markers are preserved. Typically, the `Uncased` model is better unless you know that case information is important for your task (e.g., Named Entity Recognition or Part-of-Speech tagging).\n",
        "\n",
        "When using a cased model, make sure to pass `--do_lower=False` to the training scripts.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvAsUxQ7oFye",
        "outputId": "dff2d2ae-ca89-417a-a561-6f3d7cf484ad"
      },
      "source": [
        "!gdown --id {\"1UmhRecUPwug7djN2gkNtfjDol2Pm3qhv\"} -O dpr_weights.pkl"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UmhRecUPwug7djN2gkNtfjDol2Pm3qhv\n",
            "To: /content/dpr_weights.pkl\n",
            "1.31GB [00:10, 120MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zWxzct32JJ_"
      },
      "source": [
        "# 1. Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTeBLrzz2BSH"
      },
      "source": [
        "question1 = \"How many Harry Potter books are there?\"\n",
        "question2 = \"Who is the author of Harry Potter?\"\n",
        "question3 = \"What is the name of Harry's best friends?\"\n",
        "\n",
        "passage1 = \"Harry Potter is a series of seven fantasy novels written by British author, J. K. Rowling. The novels chronicle the lives of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley, all of whom are students at Hogwarts School of Witchcraft and Wizardry. The main story arc concerns Harry's struggle against Lord Voldemort, a dark wizard who intends to become immortal, overthrow the wizard governing body known as the Ministry of Magic and subjugate all wizards and Muggles (non-magical people).\"\n",
        "passage2 = \"Since the release of the first novel, Harry Potter and the Philosopher's Stone, on 26 June 1997, the books have found immense popularity, positive reviews, and commercial success worldwide. They have attracted a wide adult audience as well as younger readers and are often considered cornerstones of modern young adult literature.[2] As of February 2018, the books have sold more than 500 million copies worldwide, making them the best-selling book series in history, and have been translated into eighty languages.[3] The last four books consecutively set records as the fastest-selling books in history, with the final installment selling roughly eleven million copies in the United States within twenty-four hours of its release.\"\n",
        "passage3 = \"The series was originally published in English by two major publishers, Bloomsbury in the United Kingdom and Scholastic Press in the United States. A play, Harry Potter and the Cursed Child, based on a story co-written by Rowling, premiered in London on 30 July 2016 at the Palace Theatre, and its script was published by Little, Brown. The original seven books were adapted into an eight-part namesake film series by Warner Bros. Pictures, which is the third highest-grossing film series of all time as of February 2020. In 2016, the total value of the Harry Potter franchise was estimated at $25 billion,[4] making Harry Potter one of the highest-grossing media franchises of all time.\"\n",
        "passage4 = \"A series of many genres, including fantasy, drama, coming of age, and the British school story (which includes elements of mystery, thriller, adventure, horror, and romance), the world of Harry Potter explores numerous themes and includes many cultural meanings and references.[5] According to Rowling, the main theme is death.[6] Other major themes in the series include prejudice, corruption, and madness.\"\n",
        "passage5 = \"The success of the books and films has allowed the Harry Potter franchise to expand with numerous derivative works, a travelling exhibition that premiered in Chicago in 2009, a studio tour in London that opened in 2012, a digital platform on which J. K. Rowling updates the series with new information and insight, and a pentalogy of spin-off films premiering in November 2016 with Fantastic Beasts and Where to Find Them, among many other developments. Most recently, themed attractions, collectively known as The Wizarding World of Harry Potter, have been built at several Universal Parks & Resorts amusement parks around the world.\"\n",
        "\n",
        "questions = [question1, question2, question3]\n",
        "passages = [passage1, passage2, passage3, passage4, passage5]\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlaIk00h2QLC"
      },
      "source": [
        "def build_input(tokenizer, sentence1, sentence2=None, max_seq_length=512):\n",
        "  \"\"\"Generate (input_ids, input_mask, segment_ids) for a single sentence.\"\"\"\n",
        "  # Tokenize and\n",
        "  tokens = tokenizer.tokenize(sentence1)\n",
        "  tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "  if sentence2:\n",
        "    len_token1 = len(tokens)\n",
        "    tokens2 = tokenizer.tokenize(sentence2)\n",
        "    len_token2 = len(tokens2)+1\n",
        "    tokens = tokens + tokens2 + [\"[SEP]\"]\n",
        "  ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  \n",
        "  # Pad the ids to max sequence length\n",
        "  pad_len = max_seq_length - len(ids)\n",
        "  input_ids = ids + [0]*pad_len\n",
        "  input_mask = [1]*len(ids) + [0]*pad_len\n",
        "\n",
        "  # Single sentence segment_ids are all 0\n",
        "  segment_ids = [0]*max_seq_length\n",
        "  if sentence2:\n",
        "    pad_len = max_seq_length - len(tokens)\n",
        "    segment_ids = [0]*len_token1+ [1]*len_token2 + [0]*pad_len\n",
        "  return (input_ids, input_mask, segment_ids)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcU9QK-a2QNZ"
      },
      "source": [
        "# Convert the sentences to bert inputs\n",
        "question_inputs = [build_input(tokenizer, s) for s in questions]\n",
        "\n",
        "# Slice to batch each input tensor\n",
        "question_input_ids = np.array([x[0] for x in question_inputs], dtype=np.int32)\n",
        "question_input_masks = np.array([x[1] for x in question_inputs], dtype=np.int32)\n",
        "question_segment_ids = np.array([x[2] for x in question_inputs], dtype=np.int32)\n",
        "\n",
        "# Convert the sentences to bert inputs\n",
        "passage_inputs = [build_input(tokenizer, s) for s in passages]\n",
        "\n",
        "# Slice to batch each input tensor\n",
        "passage_input_ids = np.array([x[0] for x in passage_inputs], dtype=np.int32)\n",
        "passage_input_masks = np.array([x[1] for x in passage_inputs], dtype=np.int32)\n",
        "passage_segment_ids = np.array([x[2] for x in passage_inputs], dtype=np.int32)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y9jLMnS98F_",
        "outputId": "9c2e0c67-b530-4a89-db02-043aefde3dc4"
      },
      "source": [
        "question_input_ids.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UE83hP99_Nv",
        "outputId": "a1632068-2815-4283-d850-b38c586ae4d7"
      },
      "source": [
        "question_input_ids"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 101, 2129, 2116, ...,    0,    0,    0],\n",
              "       [ 101, 2040, 2003, ...,    0,    0,    0],\n",
              "       [ 101, 2054, 2003, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVH8JLA9-BHc",
        "outputId": "2721da48-fc52-4622-a11e-5eac89aed153"
      },
      "source": [
        "tokenizer.convert_ids_to_tokens(question_input_ids[0][:15])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'how',\n",
              " 'many',\n",
              " 'harry',\n",
              " 'potter',\n",
              " 'books',\n",
              " 'are',\n",
              " 'there',\n",
              " '?',\n",
              " '[SEP]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n3WcySEEYtQ",
        "outputId": "27b2e544-8cca-4d9f-c0c9-7e306ab0f9d2"
      },
      "source": [
        "question_input_masks[0][:15]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glGo-uF0EfvY",
        "outputId": "2b6186d3-1d0e-42cf-f532-0c6a59eb6c72"
      },
      "source": [
        "passage_segment_ids[0][:15]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVTkmXaQ2QoQ"
      },
      "source": [
        "# 2. Retreiver model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhqDtTXhWtot"
      },
      "source": [
        "passage_encoder = hub.KerasLayer(tfhub_handle_passage_encoder)\n",
        "question_encoder = hub.KerasLayer(tfhub_handle_question_encoder)\n",
        "\n",
        "with open(\"dpr_weights.pkl\", \"rb\") as f:\n",
        "  get_passage, get_question, get_reader = pickle.load(f)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZG2fzUy238t"
      },
      "source": [
        "passage_encoder.set_weights(get_passage)\n",
        "question_encoder.set_weights(get_question)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE28kU3J23_S"
      },
      "source": [
        "passage_outputs = passage_encoder({\"input_word_ids\":passage_input_ids, \"input_mask\":passage_input_masks, \"input_type_ids\":passage_segment_ids})\n",
        "question_outputs = question_encoder({\"input_word_ids\":question_input_ids, \"input_mask\":question_input_masks, \"input_type_ids\":question_segment_ids})\n",
        "\n",
        "passage_vectors = passage_outputs['sequence_output'][:, 0, :] # [CLS]\n",
        "question_vectors = question_outputs['sequence_output'][:, 0, :]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZTBD4oE3Ihf"
      },
      "source": [
        "# 3. Find a passage !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akfCeV9v3g9p"
      },
      "source": [
        "vectors_size = 768\n",
        "index = faiss.IndexFlatL2(vectors_size)\n",
        "\n",
        "p_vectors = np.array(passage_vectors)\n",
        "q_vectors = np.array(question_vectors)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo1ZRfG_-pDD",
        "outputId": "702dee3c-0a4f-4d85-f5ab-46cb1e30dd04"
      },
      "source": [
        "print(p_vectors.shape, q_vectors.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 768) (3, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97IV8CZb3qJ4"
      },
      "source": [
        "index.add(p_vectors) "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X217JWSV3qMM",
        "outputId": "a8208c6f-1f53-4e92-f7c8-b07524be54fb"
      },
      "source": [
        "index.ntotal"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58PAzir0313b"
      },
      "source": [
        "k = 1\n",
        "D, I = index.search(q_vectors, k)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIn3r35k-2Ni",
        "outputId": "a528c5d2-04d4-45d4-c270-ee927e649c1a"
      },
      "source": [
        "D"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[66.06739 ],\n",
              "       [81.45181 ],\n",
              "       [87.274216]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KX-RIgZ-3xV",
        "outputId": "590baefb-878e-4c55-852d-6821bd0f8c42"
      },
      "source": [
        "I"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG2C6-Yl37CG"
      },
      "source": [
        "# 4. Reader model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsJwEj6a382S"
      },
      "source": [
        "reader_inputs = [build_input(tokenizer, questions[q], passages[p[0]]) for q, p in enumerate(I)]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoobglzQ4P9H"
      },
      "source": [
        "# Slice to batch each input tensor\n",
        "reader_input_ids = np.array([x[0] for x in reader_inputs], dtype=np.int32)\n",
        "reader_input_masks = np.array([x[1] for x in reader_inputs], dtype=np.int32)\n",
        "reader_segment_ids = np.array([x[2] for x in reader_inputs], dtype=np.int32)\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByYjrLsZAUaz",
        "outputId": "4d5c7ad5-fdb6-4a1c-dd37-a82d5a1c403c"
      },
      "source": [
        "print(reader_input_ids.shape)\n",
        "print(reader_input_masks.shape)\n",
        "print(reader_segment_ids.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 512)\n",
            "(3, 512)\n",
            "(3, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia0iDY9MAYDr",
        "outputId": "b8856486-6695-4f78-9379-0c43503bef38"
      },
      "source": [
        "tokenizer.convert_ids_to_tokens(reader_input_ids[0][:180])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'how',\n",
              " 'many',\n",
              " 'harry',\n",
              " 'potter',\n",
              " 'books',\n",
              " 'are',\n",
              " 'there',\n",
              " '?',\n",
              " '[SEP]',\n",
              " 'the',\n",
              " 'series',\n",
              " 'was',\n",
              " 'originally',\n",
              " 'published',\n",
              " 'in',\n",
              " 'english',\n",
              " 'by',\n",
              " 'two',\n",
              " 'major',\n",
              " 'publishers',\n",
              " ',',\n",
              " 'blooms',\n",
              " '##bury',\n",
              " 'in',\n",
              " 'the',\n",
              " 'united',\n",
              " 'kingdom',\n",
              " 'and',\n",
              " 'scholastic',\n",
              " 'press',\n",
              " 'in',\n",
              " 'the',\n",
              " 'united',\n",
              " 'states',\n",
              " '.',\n",
              " 'a',\n",
              " 'play',\n",
              " ',',\n",
              " 'harry',\n",
              " 'potter',\n",
              " 'and',\n",
              " 'the',\n",
              " 'cursed',\n",
              " 'child',\n",
              " ',',\n",
              " 'based',\n",
              " 'on',\n",
              " 'a',\n",
              " 'story',\n",
              " 'co',\n",
              " '-',\n",
              " 'written',\n",
              " 'by',\n",
              " 'row',\n",
              " '##ling',\n",
              " ',',\n",
              " 'premiered',\n",
              " 'in',\n",
              " 'london',\n",
              " 'on',\n",
              " '30',\n",
              " 'july',\n",
              " '2016',\n",
              " 'at',\n",
              " 'the',\n",
              " 'palace',\n",
              " 'theatre',\n",
              " ',',\n",
              " 'and',\n",
              " 'its',\n",
              " 'script',\n",
              " 'was',\n",
              " 'published',\n",
              " 'by',\n",
              " 'little',\n",
              " ',',\n",
              " 'brown',\n",
              " '.',\n",
              " 'the',\n",
              " 'original',\n",
              " 'seven',\n",
              " 'books',\n",
              " 'were',\n",
              " 'adapted',\n",
              " 'into',\n",
              " 'an',\n",
              " 'eight',\n",
              " '-',\n",
              " 'part',\n",
              " 'namesake',\n",
              " 'film',\n",
              " 'series',\n",
              " 'by',\n",
              " 'warner',\n",
              " 'bros',\n",
              " '.',\n",
              " 'pictures',\n",
              " ',',\n",
              " 'which',\n",
              " 'is',\n",
              " 'the',\n",
              " 'third',\n",
              " 'highest',\n",
              " '-',\n",
              " 'grossing',\n",
              " 'film',\n",
              " 'series',\n",
              " 'of',\n",
              " 'all',\n",
              " 'time',\n",
              " 'as',\n",
              " 'of',\n",
              " 'february',\n",
              " '2020',\n",
              " '.',\n",
              " 'in',\n",
              " '2016',\n",
              " ',',\n",
              " 'the',\n",
              " 'total',\n",
              " 'value',\n",
              " 'of',\n",
              " 'the',\n",
              " 'harry',\n",
              " 'potter',\n",
              " 'franchise',\n",
              " 'was',\n",
              " 'estimated',\n",
              " 'at',\n",
              " '$',\n",
              " '25',\n",
              " 'billion',\n",
              " ',',\n",
              " '[',\n",
              " '4',\n",
              " ']',\n",
              " 'making',\n",
              " 'harry',\n",
              " 'potter',\n",
              " 'one',\n",
              " 'of',\n",
              " 'the',\n",
              " 'highest',\n",
              " '-',\n",
              " 'grossing',\n",
              " 'media',\n",
              " 'franchises',\n",
              " 'of',\n",
              " 'all',\n",
              " 'time',\n",
              " '.',\n",
              " '[SEP]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp_nQiQNEqzx",
        "outputId": "e51fb255-4110-416c-f22f-f12e496e9ef3"
      },
      "source": [
        "reader_input_masks[0][:180]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc4_1fOWHO0v",
        "outputId": "0534ac0a-c1d1-4e4d-9490-96c4456d001d"
      },
      "source": [
        "reader_segment_ids[0][:180]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMkeesPK4SqL"
      },
      "source": [
        "reader_encoder = hub.KerasLayer(tfhub_handle_reader_encoder)\n",
        "reader_encoder.set_weights(get_reader[:200])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnNwlBdd9QOP"
      },
      "source": [
        "class Reader(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Reader, self).__init__()\n",
        "    self.encoder = reader_encoder\n",
        "    self.qa_outputs_weight = get_reader[-4]\n",
        "    self.qa_outputs_bias = get_reader[-3]\n",
        "    self.qa_classifier_weight = get_reader[-2]\n",
        "    self.qa_classifier_bias = get_reader[-1]\n",
        "  \n",
        "  def call(self, x):\n",
        "    batch_size = x[\"input_word_ids\"].shape[0]\n",
        "\n",
        "    x = self.encoder(x)\n",
        "    sequence_output = x[\"sequence_output\"]\n",
        "\n",
        "    qa_outputs = tf.matmul(sequence_output, self.qa_outputs_weight) + self.qa_outputs_bias\n",
        "    start_logits, end_logits = tf.split(qa_outputs, 2, axis=-1)\n",
        "    start_logits = tf.reshape(start_logits, [batch_size, -1])\n",
        "    end_logits = tf.reshape(end_logits, [batch_size, -1])\n",
        "\n",
        "    cls_output = x[\"sequence_output\"][:, 0, :]\n",
        "    relevance_logits = tf.matmul(cls_output, self.qa_classifier_weight) + self.qa_classifier_bias\n",
        "\n",
        "    return start_logits, end_logits, relevance_logits\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPEWuNSbBsAG"
      },
      "source": [
        "reader = Reader()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DAb7oFA4cUg"
      },
      "source": [
        "inputs = {\"input_word_ids\":reader_input_ids, \"input_mask\":reader_input_masks, \"input_type_ids\":reader_segment_ids}\n",
        "\n",
        "start_logits, end_logits, relevance_logits = reader(inputs)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lqWCOgJ5Hay"
      },
      "source": [
        "# 5. Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU33_mAUBM_k"
      },
      "source": [
        "def get_original(tokens_list):\n",
        "  rm_special = [token[2:] if token.startswith(\"##\") else \" \"+token for token in tokens_list]\n",
        "  text = \"\".join(rm_special)\n",
        "  return text[1:]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8g6gvh9e5Lf5",
        "outputId": "34ce61c9-cf4f-4f18-84d6-08baaf6b3ddf"
      },
      "source": [
        "for i in range(len(questions)):\n",
        "  tokens = tokenizer.convert_ids_to_tokens(reader_input_ids[i])\n",
        "\n",
        "  start_index = tf.argmax(start_logits, 1)[i].numpy()\n",
        "  end_index = tf.argmax(end_logits, 1)[i].numpy() + 1\n",
        "  print('Question : {}'.format(questions[i]))\n",
        "  print('Anwser : {}\\n'.format(get_original(tokens[start_index:end_index])))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question : How many Harry Potter books are there?\n",
            "Anwser : seven\n",
            "\n",
            "Question : Who is the author of Harry Potter?\n",
            "Anwser : j . k . rowling\n",
            "\n",
            "Question : What is the name of Harry's best friends?\n",
            "Anwser : hermione granger\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}